import os
import json
import yaml  # æ–°å¢
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# 1. è¼‰å…¥ .env ä¸¦è¨­å®š API KEY
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# 2. åˆå§‹åŒ– embedding æ¨¡å‹èˆ‡ LLM (GPT-4o-mini)
embedding_model = OpenAIEmbeddings()
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)

# 3. è¼‰å…¥ embedding çµæœ (JSON)
with open("embedding_results.json", "r", encoding="utf-8") as f:
    embedding_data = json.load(f)

# 4. è®€å– YAML æª”æ¡ˆå…§å®¹
texts = []
for item in embedding_data:
    file_path = item['file']
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # è®€å– YAML æª”æ¡ˆå¾Œè½‰æ›æˆå­—ä¸²
            yaml_content = yaml.safe_load(f)
            text_content = json.dumps(yaml_content, ensure_ascii=False, indent=2)
            texts.append(text_content)
    except Exception as e:
        print(f"ç„¡æ³•è®€å– {file_path}: {e}")
        texts.append("")

embeddings = [item['embedding'] for item in embedding_data]
faiss_db = FAISS.from_embeddings(list(zip(texts, embeddings)), embedding_model)

# 5. Prompt æ¨¡æ¿
prompt_template = """
ä½ æ˜¯ä¸€ä½ maimai éŠæˆ²å°ˆå®¶ï¼Œæ ¹æ“šä»¥ä¸‹çš„æ”»ç•¥è³‡æ–™ï¼Œå›ç­”ç©å®¶çš„å•é¡Œã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè«‹èªªã€Œé€™éƒ¨åˆ†æˆ‘ä¸ç¢ºå®šï¼Œä½†æˆ‘å¯ä»¥å¹«ä½ æŸ¥è©¢æ›´å¤šè³‡æ–™ã€‚ã€ã€‚

æ”»ç•¥è³‡æ–™ï¼š
{context}

å•é¡Œï¼š
{question}

è«‹è©³ç´°ä¸”å®Œæ•´åœ°å›ç­”ç©å®¶çš„å•é¡Œï¼Œæä¾›å…·é«”å»ºè­°èˆ‡èªªæ˜ã€‚ï¼š
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# 6. RAG æŸ¥è©¢æµç¨‹
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=faiss_db.as_retriever(search_kwargs={"k": 5}),
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

# 7. äº’å‹•å¼ QA æŸ¥è©¢
print("\næˆ‘æ˜¯ maimai LLM QA æ©Ÿå™¨äººï¼è«‹è¼¸å…¥å•é¡Œï¼Œè¼¸å…¥ 'exit' é›¢é–‹\n")

while True:
    query = input("å•é¡Œï¼š ")
    if query.lower() in ["exit", "quit", "bye"]:
        print("å†è¦‹ï¼ğŸ‘‹")
        break

    answer = qa_chain.invoke(query)
    print(answer['result'])
    print("---")
