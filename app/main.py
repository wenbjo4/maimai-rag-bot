import os
import json
from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
import gradio as gr
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# 1. è¼‰å…¥ .env ä¸¦è¨­å®š API KEY
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# 2. åˆå§‹åŒ– embedding æ¨¡å‹èˆ‡ LLM (GPT-4o)
embedding_model = OpenAIEmbeddings()
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3)

# 3. è¼‰å…¥ embedding çµæœ (JSON)
with open("data/embedding_results.json", "r", encoding="utf-8") as f:
    embedding_data = json.load(f)

# 4. å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«
texts = [item['title'] for item in embedding_data]
embeddings = [item['embedding'] for item in embedding_data]
faiss_db = FAISS.from_embeddings(list(zip(texts, embeddings)), embedding_model)

# 5. Prompt æ¨¡æ¿
prompt_template = """
ä½ æ˜¯ä¸€ä½ maimai éŠæˆ²å°ˆå®¶ï¼Œæ ¹æ“šä»¥ä¸‹çš„æ”»ç•¥è³‡æ–™ï¼Œè©³ç´°å›ç­”ç©å®¶çš„å•é¡Œã€‚
è«‹æä¾›å…·é«”çš„å»ºè­°ï¼Œä¸¦ç›¡å¯èƒ½è§£é‡‹ç›¸é—œæ¦‚å¿µã€‚
å¦‚æœè³‡æ–™ä¸­ç„¡æ³•æ‰¾åˆ°ç­”æ¡ˆï¼Œä¹Ÿå¯ä»¥æ ¹æ“šä½ å·²çŸ¥çš„è³‡è¨Šæ¨æ¸¬ï¼Œä¸¦èªªæ˜ã€‚

æ”»ç•¥è³‡æ–™ï¼š
{context}

å•é¡Œï¼š
{question}

è«‹è©³ç´°èªªæ˜ä½ çš„å›ç­”ï¼š
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# 6. RAG æŸ¥è©¢æµç¨‹
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=faiss_db.as_retriever(search_kwargs={"k": 5}),
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

# 7. Gradio UI
def rag_answer(user_query, history):
    # æ“·å–å¼•ç”¨çš„è³‡æ–™ context
    retrieved_docs = faiss_db.similarity_search(user_query, k=5)
    context = "\n".join([doc.page_content for doc in retrieved_docs])

    # æŠŠ context å‚³å…¥ prompt
    result = qa_chain.invoke({"query": user_query, "context": context})
    bot_reply = result['result']

    # æ›´æ–°å°è©±ç´€éŒ„
    history.append({"role": "user", "content": user_query})
    history.append({"role": "assistant", "content": bot_reply + "\n\nğŸ” å¼•ç”¨è³‡æ–™:\n" + context})
    return "", history, history


gr_app = gr.Blocks()
with gr.Blocks() as gr_app:
    gr.Markdown("# ğŸ® maimai RAG å•ç­”æ©Ÿå™¨äºº")
    chatbot = gr.Chatbot(type='messages')
    user_input = gr.Textbox(label="è«‹è¼¸å…¥ä½ çš„å•é¡Œ", placeholder="ä¾‹å¦‚ï¼šKOPæ˜¯ä»€éº¼ï¼Ÿ")
    submit_btn = gr.Button("é€å‡ºå•é¡Œ")
    state = gr.State([])

    def disable_input():
        return gr.update(interactive=False)

    def enable_input():
        return gr.update(interactive=True)

    submit_btn.click(disable_input, None, [user_input]) \
        .then(rag_answer, [user_input, state], [user_input, chatbot, state]) \
        .then(enable_input, None, [user_input])

    user_input.submit(rag_answer, [user_input, state], [user_input, chatbot, state])

if __name__ == "__main__":
    gr_app.launch(server_name="0.0.0.0", server_port=8000)
